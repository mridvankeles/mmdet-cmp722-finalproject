import cv2
import json
import torch
import xml.etree.ElementTree as ET
from tqdm import tqdm
from torchvision.ops import nms
import pickle
import numpy as np
from typing import List, Dict, Any, Tuple
import os
import glob
from mmdet.apis import init_detector, inference_detector
from copy import deepcopy


def tile_unannotated_image_folder(
    input_dir: str,
    output_dir: str,
    tile_w: int = 800,
    tile_h: int = 800,
    stride: int = 200
) -> Tuple[Dict[int, Tuple[str, int, int]], List[Dict[str, Any]]]:
    """Tile images in a folder and return offset map and tile metadata."""
    os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)

    stride_w = stride
    stride_h = stride

    offset_map: Dict[int, Tuple[str, int, int]] = {}
    tiled_images_metadata: List[Dict[str, Any]] = []
    img_id = 1

    original_files = sorted(glob.glob(os.path.join(input_dir, "*.*")))
    original_files = [f for f in original_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    for filepath in tqdm(original_files, desc="Tiling Unannotated Images"):
        filename = os.path.basename(filepath)
        img = cv2.imread(filepath)
        if img is None:
            continue

        height, width = img.shape[:2]

        for y in range(0, height, stride_h):
            for x in range(0, width, stride_w):
                y_end = min(y + tile_h, height)
                x_end = min(x + tile_w, width)

                crop = img[y:y_end, x:x_end]
                ch, cw = crop.shape[:2]

                if ch < tile_h or cw < tile_w:
                    crop = cv2.copyMakeBorder(
                        crop, 0, tile_h - ch, 0, tile_w - cw,
                        cv2.BORDER_CONSTANT, value=(0, 0, 0)
                    )

                new_filename = f"{img_id:06d}.jpg"
                cv2.imwrite(os.path.join(output_dir, "images", new_filename), crop)

                tiled_images_metadata.append({
                    "id": img_id,
                    "file_name": new_filename,
                    "width": tile_w,
                    "height": tile_h,
                    "original_file": filename,
                    "offset_x": x,
                    "offset_y": y
                })
                offset_map[img_id] = (filename, x, y)
                img_id += 1

    print(f"\n Tiling complete. {len(tiled_images_metadata)} tiles saved.")
    print(f"Offset map contains data for {len(offset_map)} tiles.")
    return offset_map, tiled_images_metadata

def tile_coco_annotations(
    coco_json_path: str,
    offset_map_path: str,
    tiling_metadata_path: str,
    output_json_path: str
):
    """
    Create a tiled COCO annotation file using an original COCO annotation file
    and metadata generated from tile_unannotated_image_folder().

    Args:
        coco_json_path: Path to the original COCO JSON file.
        offset_map_path: Path to offset_map.json (dict of {id: [orig_filename, offset_x, offset_y]}).
        tiling_metadata_path: Path to tiled_metadata.json (list of dicts as generated by tile_unannotated_image_folder()).
        output_json_path: Where to save the tiled COCO JSON file.
    """
    # ---- Load all input JSON files ----
    with open(coco_json_path, "r") as f:
        coco = json.load(f)
    with open(offset_map_path, "r") as f:
        offset_map = json.load(f)
    
    with open(tiling_metadata_path, "r") as f:
        tiled_metadata = json.load(f)

    if isinstance(tiled_metadata, dict):
        tiled_metadata = list(tiled_metadata.values())

    print(f"Loaded {len(coco['images'])} images, {len(coco['annotations'])} annotations from COCO.")
    print(f"Loaded {len(offset_map)} tiles from offset_map.")
    print(f"Loaded {len(tiled_metadata)} tile metadata entries.")


    # ---- Build helper mappings ----
    filename_to_imgid = {img["file_name"]: img["id"] for img in coco["images"]}
    imgid_to_anns = {}
    for ann in coco["annotations"]:
        imgid_to_anns.setdefault(ann["image_id"], []).append(ann)

    # ---- Initialize new COCO structure ----
    print("COCOCATEGO:", coco["categories"])
    new_coco = {
        "images": [],
        "annotations": [],
        "categories": coco["categories"]
    }

    

    ann_id = 1

    # ---- Process each tile ----
    for i, tile_meta in enumerate(tiled_metadata):
        if not isinstance(tile_meta, dict):
            print(f"\n--- DEBUG ERROR ---\nElement at index {i} is NOT a dict. Type: {type(tile_meta)}\nValue: {tile_meta}\n-------------------\n")
            continue

        tile_id = tile_meta["id"] # 
        tile_filename = tile_meta["file_name"]
        original_filename = tile_meta["original_file"]
        off_x = tile_meta["offset_x"]
        off_y = tile_meta["offset_y"]
        tile_w = tile_meta["width"]
        tile_h = tile_meta["height"]

        # Add image entry for this tile
        new_coco["images"].append({
            "id": tile_id,
            "file_name": tile_filename,
            "width": tile_w,
            "height": tile_h
        })

        # Get original image ID
        orig_img_id = filename_to_imgid.get(original_filename)

        if orig_img_id is None:
            continue

        original_annotations = imgid_to_anns.get(orig_img_id, [])
        for ann in original_annotations:
            x, y, w, h = ann["bbox"]

            # ---- Check overlap between bbox and tile ----
            if (x + w < off_x) or (x > off_x + tile_w) or (y + h < off_y) or (y > off_y + tile_h):
                continue  # no overlap

            # ---- Clip bbox within tile boundaries ----
            new_xmin = max(0, x - off_x)
            new_ymin = max(0, y - off_y)
            new_xmax = min(tile_w, x + w - off_x)
            new_ymax = min(tile_h, y + h - off_y)

            new_w = new_xmax - new_xmin
            new_h = new_ymax - new_ymin
            if new_w <= 1 or new_h <= 1:
                continue

            # ---- Build new annotation ----
            new_ann = deepcopy(ann)
            new_ann["id"] = ann_id
            ann_id += 1
            new_ann["image_id"] = tile_id
            new_ann["bbox"] = [new_xmin, new_ymin, new_w, new_h]
            new_ann["area"] = new_w * new_h

            # ---- Clip segmentation if available ----
            if "segmentation" in ann and isinstance(ann["segmentation"], list):
                new_seg = []
                for seg in ann["segmentation"]:
                    new_points = []
                    for i in range(0, len(seg), 2):
                        px, py = seg[i], seg[i + 1]
                        if off_x <= px < off_x + tile_w and off_y <= py < off_y + tile_h:
                            new_points.extend([px - off_x, py - off_y])
                    if len(new_points) >= 6:  # valid polygon
                        new_seg.append(new_points)
                new_ann["segmentation"] = new_seg

            new_coco["annotations"].append(new_ann)

    # ---- Save new tiled COCO ----
    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
    with open(output_json_path, "w") as f:
        json.dump(new_coco, f, indent=2)

    print(f"\n Tiled COCO saved to: {output_json_path}")
    print(f" Images: {len(new_coco['images'])}, Annotations: {len(new_coco['annotations'])}")


def autolabel_predictions_to_coco_json(
    final_preds: Dict[str, List[Dict[str, Any]]],
    category_label_map: Dict[int, str],
    original_image_folder: str,
    output_json_path: str
) -> Dict[str, Any]:
    """Export stitched predictions as a full COCO file (images+annotations+categories)."""
    categories = [{"id": k, "name": v} for k, v in category_label_map.items()]

    new_coco_annotations: Dict[str, Any] = {
        "info": {"description": "Autolabeled predictions generated from model inference"},
        "licenses": [],
        "categories": categories,
        "images": [],
        "annotations": []
    }

    print("Building image metadata by reading file dimensions...")
    image_id_counter = 1
    filename_to_id: Dict[str, int] = {}

    files_to_process = set(final_preds.keys())

    for filename in tqdm(files_to_process, desc="Reading Image Sizes"):
        filepaths = glob.glob(os.path.join(original_image_folder, "**", filename), recursive=True)
        if not filepaths:
            print(f"Warning: Image file not found for prediction: {filename}. Skipping.")
            continue

        img = cv2.imread(filepaths[0])
        if img is None:
            continue

        height, width = img.shape[:2]
        image_id = image_id_counter
        filename_to_id[filename] = image_id

        new_coco_annotations['images'].append({
            "id": image_id,
            "width": width,
            "height": height,
            "file_name": filename,
            "license": 0,
            "date_captured": ""
        })
        image_id_counter += 1

    annotation_id_counter = 1
    print("Formatting predictions into annotations...")

    for file_name, detections in tqdm(final_preds.items(), desc="Formatting Annotations"):
        if file_name not in filename_to_id:
            continue
        image_id = filename_to_id[file_name]
        for d in detections:
            bbox = [float(val) for val in d['bbox']]
            w, h = bbox[2], bbox[3]
            new_coco_annotations['annotations'].append({
                "id": annotation_id_counter,
                "image_id": image_id,
                "category_id": int(d['category_id']),
                "segmentation": [],
                "area": w * h,
                "bbox": bbox,
                "iscrowd": 0,
                "score": np.round(float(d['score']), 2)
            })
            annotation_id_counter += 1

    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
    with open(output_json_path, 'w') as f:
        json.dump(new_coco_annotations, f, indent=4)

    print(f"\n Converted {len(new_coco_annotations['annotations'])} predictions into COCO annotations.")
    print(f"New COCO annotation file saved to: {output_json_path}")
    return new_coco_annotations


def run_mmdet_inference_on_folder(
    config_path: str,
    checkpoint_path: str,
    image_folder: str,
    output_pickle_path: str,
    device: str = 'cuda:0',
    cfg_options: Dict[str, Any] = None
) -> List[List[np.ndarray]]:
    """Load model, run inference on images in a folder, save results pickle."""
    print(f"Loading model from config: {config_path}")
    try:
        model = init_detector(config_path, checkpoint_path, device=device, cfg_options=cfg_options)
    except Exception as e:
        print(f"Error initializing model: {e}")
        print("Check your config/checkpoint and MMDetection install.")
        return []

    image_paths = sorted(glob.glob(os.path.join(image_folder, "*.jpg")))
    if not image_paths:
        print(f"Warning: No JPG images found in {image_folder}")
        return []

    print(f"Found {len(image_paths)} images. Starting inference...")
    all_results: List[List[np.ndarray]] = []

    for img_path in tqdm(image_paths, desc="Running Inference"):
        result = inference_detector(model, img_path)
        if isinstance(result, tuple):
            bbox_results = result[0]
        else:
            bbox_results = result
        all_results.append(bbox_results)

    os.makedirs(os.path.dirname(output_pickle_path), exist_ok=True)
    with open(output_pickle_path, 'wb') as f:
        pickle.dump(all_results, f)

    print(f"\nInference complete. Results saved to: {output_pickle_path}")
    return all_results


def convert_to_coco_list(mmdet_results: List[List[np.ndarray]]) -> List[Dict[str, Any]]:
    """Convert MMDetection results (per-class arrays) to COCO result list."""
    coco_list: List[Dict[str, Any]] = []
    for image_id, img_preds_per_class in enumerate(mmdet_results, start=1):
        for category_id, class_detections in enumerate(img_preds_per_class, start=0):
            if class_detections.size == 0:
                continue
            for detection in class_detections:
                x1, y1, x2, y2, score = detection
                w = x2 - x1
                h = y2 - y1
                coco_list.append({
                    "image_id": image_id,
                    "bbox": [float(x1), float(y1), float(w), float(h)],
                    "score": float(score),
                    "category_id": category_id
                })
    print(f"Successfully converted {len(coco_list)} total detections.")
    return coco_list


def get_all_original_image_dimensions(
    image_folder: str,
    recursive: bool = False
) -> Dict[str, Tuple[int, int]]:
    """Cache (width, height) for all images in a folder."""
    search_pattern = os.path.join(image_folder, '**' if recursive else '', '*')
    image_files: List[str] = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp',".JPG"]:
        image_files.extend(glob.glob(search_pattern + ext, recursive=recursive))

    dimension_map: Dict[str, Tuple[int, int]] = {}
    print(f"Caching dimensions for {len(image_files)} original images...")
    for filepath in tqdm(image_files, desc="Reading Dimensions"):
        filename = os.path.basename(filepath)
        img = cv2.imread(filepath)
        if img is not None:
            height, width = img.shape[:2]
            dimension_map[filename] = (width, height)
    print(f" Finished caching dimensions for {len(dimension_map)} images.")
    return dimension_map


def stitch_predictions(
    preds: List[Dict[str, Any]],
    offset_map: Dict[int, Tuple[str, int, int]],
    dimension_map: Dict[str, Tuple[int, int]],
    iou_thresh: float = 0.9
) -> Dict[str, List[Dict[str, Any]]]:
    """Stitch tiled predictions to original coords, clip to borders, apply class-aware NMS."""
    stitched: Dict[str, Dict[str, list]] = {}

    for p in preds:
        patch_id = p["image_id"]
        if patch_id not in offset_map:
            continue
        orig_file, off_x, off_y = offset_map[patch_id]
        x, y, w, h = p["bbox"]
        gx1, gy1 = x + off_x, y + off_y
        gx2, gy2 = gx1 + w, gy1 + h

        if orig_file not in dimension_map:
            continue
        width, height = dimension_map[orig_file]
        gx1 = max(0.0, gx1)
        gy1 = max(0.0, gy1)
        gx2 = min(float(width), gx2)
        gy2 = min(float(height), gy2)
        if gx2 <= gx1 + 1e-4 or gy2 <= gy1 + 1e-4:
            continue

        if orig_file not in stitched:
            stitched[orig_file] = {"boxes": [], "scores": [], "labels": []}
        stitched[orig_file]["boxes"].append([gx1, gy1, gx2, gy2])
        stitched[orig_file]["scores"].append(p["score"])
        stitched[orig_file]["labels"].append(p["category_id"])

    final_results: Dict[str, List[Dict[str, Any]]] = {}
    for fname, data in stitched.items():
        if not data["boxes"]:
            continue
        boxes = torch.tensor(data["boxes"], dtype=torch.float32)
        scores = torch.tensor(data["scores"])
        labels = torch.tensor(data["labels"])
        image_detections: List[Dict[str, Any]] = []
        for label_id in torch.unique(labels):
            class_mask = (labels == label_id)
            class_boxes = boxes[class_mask]
            class_scores = scores[class_mask]
            if class_boxes.shape[0] == 0:
                continue
            keep_indices = nms(class_boxes, class_scores, iou_thresh)
            final_boxes_class = class_boxes[keep_indices].numpy().tolist()
            final_scores_class = class_scores[keep_indices].numpy().tolist()
            for b, s in zip(final_boxes_class, final_scores_class):
                x1, y1, x2, y2 = b
                image_detections.append({
                    "bbox": [x1, y1, x2 - x1, y2 - y1],
                    "score": s,
                    "category_id": int(label_id.item())
                })
        final_results[fname] = image_detections

    return final_results


if __name__ == "__main__":
    pass

    
        


    